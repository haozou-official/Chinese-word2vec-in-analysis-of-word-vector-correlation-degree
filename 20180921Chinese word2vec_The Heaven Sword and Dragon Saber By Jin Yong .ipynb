{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此函数作用是对初始语料进行分词处理后，作为训练模型的语料\n",
    "def cut_txt(old_file):\n",
    "    import jieba\n",
    "    global cut_file     # 分词之后保存的文件名\n",
    "    cut_file = old_file + '_cut.txt'\n",
    "\n",
    "    try:\n",
    "        fi = open(old_file, 'r', encoding='utf-8')\n",
    "    except BaseException as e:  # 因BaseException是所有错误的基类，用它可以获得所有错误类型\n",
    "        print(Exception, \":\", e)    # 追踪错误详细信息\n",
    "\n",
    "    text = fi.read()  # 获取文本内容\n",
    "    new_text = jieba.cut(text, cut_all=False)  # 精确模式\n",
    "    str_out = ' '.join(new_text).replace('，', '').replace('。', '').replace('？', '').replace('！', '') \\\n",
    "        .replace('“', '').replace('”', '').replace('：', '').replace('…', '').replace('（', '').replace('）', '') \\\n",
    "        .replace('—', '').replace('《', '').replace('》', '').replace('、', '').replace('‘', '') \\\n",
    "        .replace('’', '')     # 去掉标点符号\n",
    "    fo = open(cut_file, 'w', encoding='utf-8')\n",
    "    fo.write(str_out)\n",
    "    \n",
    "def model_train(train_file_name, save_model_file):  # model_file_name为训练语料的路径,save_model为保存模型名\n",
    "    from gensim.models import word2vec\n",
    "    import gensim\n",
    "    import logging\n",
    "    # 模型训练，生成词向量\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    sentences = word2vec.Text8Corpus(train_file_name)  # 加载语料\n",
    "    model = gensim.models.Word2Vec(sentences, size=200)  # 训练skip-gram模型; 默认window=5\n",
    "    model.save(save_model_file)\n",
    "    model.wv.save_word2vec_format(save_model_name + \".bin\", binary=True)   # 以二进制类型保存模型以便重用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec模型调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-21 11:48:32,859 : INFO : collecting all words and their counts\n",
      "2018-09-21 11:48:32,863 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-09-21 11:48:33,142 : INFO : collected 46611 word types from a corpus of 485762 raw words and 49 sentences\n",
      "2018-09-21 11:48:33,143 : INFO : Loading a fresh vocabulary\n",
      "2018-09-21 11:48:33,234 : INFO : effective_min_count=5 retains 10415 unique words (22% of original 46611, drops 36196)\n",
      "2018-09-21 11:48:33,235 : INFO : effective_min_count=5 leaves 429233 word corpus (88% of original 485762, drops 56529)\n",
      "2018-09-21 11:48:33,280 : INFO : deleting the raw counts dictionary of 46611 items\n",
      "2018-09-21 11:48:33,288 : INFO : sample=0.001 downsamples 40 most-common words\n",
      "2018-09-21 11:48:33,289 : INFO : downsampling leaves estimated 365095 word corpus (85.1% of prior 429233)\n",
      "2018-09-21 11:48:33,341 : INFO : estimated required memory for 10415 words and 200 dimensions: 21871500 bytes\n",
      "2018-09-21 11:48:33,343 : INFO : resetting layer weights\n",
      "2018-09-21 11:48:33,510 : INFO : training model with 3 workers on 10415 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-09-21 11:48:34,018 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-21 11:48:34,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-21 11:48:34,031 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-21 11:48:34,032 : INFO : EPOCH - 1 : training on 485762 raw words (365136 effective words) took 0.5s, 702895 effective words/s\n",
      "2018-09-21 11:48:34,760 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-21 11:48:34,771 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-21 11:48:34,778 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-21 11:48:34,779 : INFO : EPOCH - 2 : training on 485762 raw words (364907 effective words) took 0.7s, 493939 effective words/s\n",
      "2018-09-21 11:48:35,401 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-21 11:48:35,404 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-21 11:48:35,407 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-21 11:48:35,408 : INFO : EPOCH - 3 : training on 485762 raw words (365243 effective words) took 0.6s, 583461 effective words/s\n",
      "2018-09-21 11:48:35,972 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-21 11:48:35,982 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-21 11:48:35,986 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-21 11:48:35,987 : INFO : EPOCH - 4 : training on 485762 raw words (364753 effective words) took 0.6s, 634456 effective words/s\n",
      "2018-09-21 11:48:36,536 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-09-21 11:48:36,546 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-09-21 11:48:36,549 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-09-21 11:48:36,550 : INFO : EPOCH - 5 : training on 485762 raw words (365043 effective words) took 0.6s, 652326 effective words/s\n",
      "2018-09-21 11:48:36,551 : INFO : training on a 2428810 raw words (1825082 effective words) took 3.0s, 600258 effective words/s\n",
      "2018-09-21 11:48:36,552 : INFO : saving Word2Vec object under 倚天屠龙记1.model, separately None\n",
      "2018-09-21 11:48:36,555 : INFO : not storing attribute vectors_norm\n",
      "2018-09-21 11:48:36,557 : INFO : not storing attribute cum_table\n",
      "2018-09-21 11:48:36,852 : INFO : saved 倚天屠龙记1.model\n",
      "2018-09-21 11:48:36,853 : INFO : storing 10415x200 projection weights into 倚天屠龙记1.model.bin\n",
      "2018-09-21 11:48:37,059 : INFO : loading Word2Vec object from 倚天屠龙记1.model\n",
      "2018-09-21 11:48:37,374 : INFO : loading vocabulary recursively from 倚天屠龙记1.model.vocabulary.* with mmap=None\n",
      "2018-09-21 11:48:37,375 : INFO : loading wv recursively from 倚天屠龙记1.model.wv.* with mmap=None\n",
      "2018-09-21 11:48:37,376 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-09-21 11:48:37,376 : INFO : loading trainables recursively from 倚天屠龙记1.model.trainables.* with mmap=None\n",
      "2018-09-21 11:48:37,379 : INFO : setting ignored attribute cum_table to None\n",
      "2018-09-21 11:48:37,380 : INFO : loaded 倚天屠龙记1.model\n",
      "/Users/zouhao/anaconda3/envs/python35/lib/python3.5/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "/Users/zouhao/anaconda3/envs/python35/lib/python3.5/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n",
      "/Users/zouhao/anaconda3/envs/python35/lib/python3.5/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "2018-09-21 11:48:37,427 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "赵敏和韦一笑的相似度为： 0.7938857\n",
      "-------------------------------\n",
      "\n",
      "和张三丰最相关的词有：\n",
      "\n",
      "金花婆婆 0.9892849922180176\n",
      "张松溪 0.9846320152282715\n",
      "四弟 0.9816751480102539\n",
      "赵敏格 0.9814869165420532\n",
      "可行 0.9795811176300049\n",
      "事不宜迟 0.9795102477073669\n",
      "灭绝师太 0.9793342351913452\n",
      "寿南山 0.978480339050293\n",
      "通道 0.977789580821991\n",
      "宋远桥 0.9776178002357483\n",
      "-------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import os\n",
    "import gensim\n",
    "\n",
    "# if not os.path.exists(cut_file):    # 判断文件是否存在，参考：https://www.cnblogs.com/jhao/p/7243043.html\n",
    "cut_txt('./The Heaven Sword and Dragon Saber By Jin Yong.txt')  # 须注意文件必须先另存为utf-8编码格式\n",
    "\n",
    "save_model_name = 'The Heaven Sword and Dragon Saber By Jin Yong.model'\n",
    "if not os.path.exists(save_model_name):     # 判断文件是否存在\n",
    "    model_train(cut_file, save_model_name)\n",
    "else:\n",
    "    print('此训练模型已经存在，不用再次训练')\n",
    "\n",
    "# 加载已训练好的模型\n",
    "model_1 = word2vec.Word2Vec.load(save_model_name)\n",
    "# 计算两个词的相似度/相关程度\n",
    "y1 = model_1.similarity(\"赵敏\", \"韦一笑\")\n",
    "print(u\"赵敏和韦一笑的相似度为：\", y1)\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "# 计算某个词的相关词列表\n",
    "y2 = model_1.most_similar(\"张三丰\", topn=10)  # 10个最相关的\n",
    "print(u\"和张三丰最相关的词有：\\n\")\n",
    "for item in y2:\n",
    "    print(item[0], item[1])\n",
    "print(\"-------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
